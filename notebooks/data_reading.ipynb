{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from pep_con_tool.data_processing.arrow_funtions import read_csv_file, read_csv_headers, pa_to_df\n",
    "from pep_con_tool.data_processing.pd_functions import get_columns_containing_string, date_column_parser,fix_date_columns, set_col_cat\n",
    "import janitor\n",
    "from janitor import clean_names\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    un = len(df[col].unique())\n",
    "    if un < unique_n:\n",
    "        #set dtype to category\n",
    "        df[col] = df[col].astype('category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"col_1\": [\"a\", \"b\", \"b\"], \"col_2\": [\"a\", \"b\", \"c\"]})\n",
    "df = set_col_cat(df, 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "col_1    object\n",
       "col_2    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df[\"col_1\"].dtype == \"category\"\n",
    "assert df[\"col_1\"][0] == \"a\"\n",
    "assert df[\"col_1\"][1] == \"b\"\n",
    "assert df[\"col_1\"][2] == \"c\"\n",
    "assert df[\"col_2\"].dtype != \"category\"\n",
    "assert df[\"col_2\"][0] == \"a\"\n",
    "assert df[\"col_2\"][1] == \"b\"\n",
    "assert df[\"col_2\"][2] == \"c\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"test for fix_date_columns\"\"\"\n",
    "df = pd.DataFrame({\"col_1\": [\"2020-01-01\", \"2020-01-02\", \"2020-01-03\"], \"col_2\": [\"2020-01-01\", \"2020-01-02\", \"2020-01-03\"]})\n",
    "df = fix_date_columns(df, \"col\", \"%Y-%m-%d\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['col_1', 'col_2']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_columns_containing_string(df, \"col\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2020-01-01'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"col_1\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10088\\3412812865.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32massert\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"col_1\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"datetime64[ns]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32massert\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"col_1\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTimestamp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"2020-01-01\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32massert\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"col_1\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTimestamp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"2020-01-02\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32massert\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"col_1\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTimestamp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"2020-01-03\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert df[\"col_1\"].dtype == \"datetime64[ns]\"\n",
    "assert df[\"col_1\"][0] == pd.Timestamp(\"2020-01-01\")\n",
    "assert df[\"col_1\"][1] == pd.Timestamp(\"2020-01-02\")\n",
    "assert df[\"col_1\"][2] == pd.Timestamp(\"2020-01-03\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data\n",
    "\n",
    "### step 1: get headers, get data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = \"../data/csv_input/2020_01.csv\"\n",
    "filen = \"../tests/test_files/test_csv_1.csv\"\n",
    "\n",
    "Filename = files\n",
    "delimiter = \";\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv file to pyarrow table, then convert to pandas df\n",
    "def csv2df(Filename, delimiter=\";\"):\n",
    "    \"\"\"read csv file to pyarrow table, add filename to column\"\"\"\n",
    "    headers = read_csv_headers(Filename, delimiter)\n",
    "    pa_table = read_csv_file(Filename, headers, 1, delimiter, True)\n",
    "    df = pa_to_df(pa_table)\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas show all columns in jupyter notebook\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_columns_containing_string(df, string):\n",
    "    \"\"\" get date columns from name \"\"\"\n",
    "    return [col for col in df.columns if string in col.lower()]\n",
    "\n",
    "# parse date column function\n",
    "def date_column_parser(df, date_column, date_format):\n",
    "    \"\"\"\n",
    "    parse date column function\n",
    "    df: pandas dataframe\n",
    "    date_column: date column name\n",
    "    date_format: date format\n",
    "    \"\"\"\n",
    "    df[date_column] = pd.to_datetime(df[date_column], format=date_format, errors='coerce')\n",
    "    return df  \n",
    "\n",
    "def fix_date_columns(df, date_format, column_string):\n",
    "    \"\"\" \n",
    "    fix date columns \n",
    "    df: pandas dataframe\n",
    "    date_format: date format example: '%Y-%m-%d'\n",
    "    column_string: string to search for in column names\n",
    "    \"\"\"\n",
    "\n",
    "    date_columns = get_columns_containing_string(df, column_string)\n",
    "\n",
    "    for col in date_columns:\n",
    "        df = date_column_parser(df, col, date_format)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# function to add date and time columns\n",
    "def add_date_time_columns(df, dt_col : list, remove_original : bool = True):\n",
    "    \"\"\" \n",
    "    function to add date and time columns \n",
    "    df: pandas dataframe\n",
    "    dt_col: list of tuples [(date_col, time_col), (date_col, time_col)]\n",
    "    \"\"\"\n",
    "\n",
    "    for (date_column, time_column) in dt_col:\n",
    "        df[(f\"{date_column}_time\")] = df[date_column] + (df[time_column] - datetime(1900, 1, 1))\n",
    "    \n",
    "    if remove_original:\n",
    "        # remove dt_col columns\n",
    "        for (date_column, time_column) in dt_col:\n",
    "            df.drop([date_column, time_column], axis=1, inplace=True)\n",
    "            \n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "def set_col_cat(df, unique_n=10):\n",
    "    \"\"\"\n",
    "    Looping through all the columns in the dataframe and checking the number of unique values in each\n",
    "    column. \n",
    "    If the number of unique values is less than the unique_n argument\n",
    "    it sets the column data type to category.\n",
    "    \"\"\"    \n",
    "    \n",
    "    for col in df.columns:\n",
    "        un = len(df[col].unique())\n",
    "        if un < unique_n:\n",
    "            #set dtype to category\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    return df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv file to pandas df\n",
    "df = csv2df(files)\n",
    "\n",
    "# clean up data frame\n",
    "\n",
    "#clean column names:\n",
    "df = df.clean_names()\n",
    "\n",
    "# formatting date\n",
    "date_format = '%d.%m.%Y'\n",
    "column_string = 'date'\n",
    "\n",
    "df = fix_date_columns(df, date_format, column_string)\n",
    "\n",
    "# formatting time columns\n",
    "# set date_format function to format the time column.\n",
    "date_format = '%H:%M:%S'\n",
    "column_string = 'time'\n",
    "\n",
    "df = fix_date_columns(df, date_format, column_string)\n",
    "\n",
    "# combine date and time columns\n",
    "dt_col = [('task_beginning_date', 'task_beginning_time'), ('task_ending_date',\t'task_ending_time')]\n",
    "df = add_date_time_columns(df, dt_col)\n",
    "\n",
    "df = set_col_cat(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_pipe_pba(csv_file):\n",
    "    \"\"\"data cleaning pipeline for reading PBA csv files\"\"\"\n",
    "\n",
    "    # formatting date\n",
    "    date_format = \"%d.%m.%Y\"\n",
    "    date_string = \"date\"\n",
    "\n",
    "    # formatting time columns\n",
    "    # set date_format function to format the time column.\n",
    "    time_format = \"%H:%M:%S\"\n",
    "    time_string = \"time\"\n",
    "\n",
    "    # combine date and time columns\n",
    "    dt_col = [\n",
    "        (\"task_beginning_date\", \"task_beginning_time\"),\n",
    "        (\"task_ending_date\", \"task_ending_time\"),\n",
    "    ]\n",
    "\n",
    "    df = (\n",
    "        csv2df(csv_file)\n",
    "        .pipe(clean_names)\n",
    "        .pipe(fix_date_columns, date_format, date_string)\n",
    "        .pipe(fix_date_columns, time_format, time_string)\n",
    "        .pipe(add_date_time_columns, dt_col)\n",
    "        .pipe(set_col_cat)\n",
    "    )\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test for read_csv_file\n",
    "def test_read_csv_file():\n",
    "    \"\"\"test for the read_csv_file function\"\"\"\n",
    "    # read_csv_file(\"test_data/test_data.csv\")\n",
    "    headers = read_csv_headers(CSV_TEST_C_FILE_1, \",\")\n",
    "    assert read_csv_file(CSV_TEST_C_FILE_1, headers, 1, \",\") == [\n",
    "        {\"a\": \"1\", \"b\": \"2\", \"c\": \"3\"},\n",
    "        {\"a\": \"4\", \"b\": \"5\", \"c\": \"6\"},\n",
    "    ]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_TEST_C_FILE_1 = \"../tests/test_files/test_csv_1.csv\"\n",
    "headers = read_csv_headers(CSV_TEST_C_FILE_1, \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyarrow_table = read_csv_file(CSV_TEST_C_FILE_1, headers, 1, \",\")\n",
    "assert type(pyarrow_table) == pa.Table\n",
    "# pa table headers include filename\n",
    "assert headers == list(pyarrow_table.column_names[:-1])\n",
    "\n",
    "firstrow = {'header_0': 'row_0_col_0',\n",
    " 'header_1': 'row_0_col_1',\n",
    " 'header_2': 'row_0_col_2',\n",
    " 'header_3': 'row_0_col_3',\n",
    " 'header_4': 'row_0_col_4',\n",
    " 'header_5': 'row_0_col_5',\n",
    " 'header_6': 'row_0_col_6',\n",
    " 'header_7': 'row_0_col_7',\n",
    " 'header_8': 'row_0_col_8',\n",
    " 'header_9': 'row_0_col_9',\n",
    " 'header_10': 'row_0_col_10',\n",
    " 'header_11': 'row_0_col_11',\n",
    " 'header_12': 'row_0_col_12',\n",
    " 'header_13': 'row_0_col_13',\n",
    " 'header_14': 'row_0_col_14',\n",
    " 'header_15': 'row_0_col_15',\n",
    " 'filename': 'test_csv_1'}\n",
    "\n",
    "assert pyarrow_table.to_pylist()[0] == firstrow\n",
    "\n",
    "firstcol = ['row_0_col_0',\n",
    " 'row_1_col_0',\n",
    " 'row_2_col_0',\n",
    " 'row_3_col_0',\n",
    " 'row_4_col_0',\n",
    " 'row_5_col_0',\n",
    " 'row_6_col_0',\n",
    " 'row_7_col_0',\n",
    " 'row_8_col_0',\n",
    " 'row_9_col_0',\n",
    " 'row_10_col_0',\n",
    " 'row_11_col_0',\n",
    " 'row_12_col_0']\n",
    "\n",
    "assert pyarrow_table.to_pydict()[headers[0]] == firstcol\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['row_0_col_0',\n",
       " 'row_1_col_0',\n",
       " 'row_2_col_0',\n",
       " 'row_3_col_0',\n",
       " 'row_4_col_0',\n",
       " 'row_5_col_0',\n",
       " 'row_6_col_0',\n",
       " 'row_7_col_0',\n",
       " 'row_8_col_0',\n",
       " 'row_9_col_0',\n",
       " 'row_10_col_0',\n",
       " 'row_11_col_0',\n",
       " 'row_12_col_0']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get first row of pa table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'b_1', 'c', 'c_1', 'c_1']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6db3eaa6955d55cc63e3800f2fa75c661f35370a09e058cbf6fd7b14717743c3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
